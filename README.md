# ICM Algorithm 1 Implementation - TruthfulQA

Implementation of Algorithm 1 from "Unsupervised Elicitation of Language Models" (Wen et al., 2025).

## Setup

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Requirements:
- openai
- numpy
- matplotlib
- seaborn
- tqdm

### 2. Set API Key

```bash
export HYPERBOLIC_API_KEY="your-api-key-here"
```

Get your API key from https://www.hyperbolic.ai/

## Running the Code

### Quick Test (Recommended First)

Test with small dataset to verify everything works:

```bash
python run_icm.py \
  --model meta-llama/Meta-Llama-3.1-405B \
  --train_samples 30 \
  --test_samples 20 \
  --max_icm_iters 100
```

This takes ~10 minutes.

### Full Run

Run on complete dataset (256 train, 100 test):

```bash
python run_icm.py --model meta-llama/Meta-Llama-3.1-405B
```

This takes ~30-45 minutes.

## What the Code Does

1. **Step 1:** Runs ICM algorithm on training data to generate labels
2. **Step 2:** Uses generated labels to predict test data
3. **Step 3:** Compares to baselines (Random, Zero-shot)
4. **Step 4:** Generates four-bar comparison graph

## Output

Each run creates a directory in `runs/` with timestamp and parameters:

```
runs/20251106_120000_model-Meta-Llama-3.1-405B_k6_train30_test20_iters100_seed42/
├── results.json                  # Summary metrics
├── test_predictions.json         # All test predictions  
├── icm_generated_labels.json     # Labels generated by ICM
├── four_bars.png                 # Comparison graph
└── run_metadata.json             # Run configuration
```

A symlink `runs/latest/` always points to the most recent run.

### Viewing Results

```bash
# View summary
cat runs/latest/results.json

# View graph (copy to local machine if on SSH)
runs/latest/four_bars.png
```

## Command Line Options

```
--model           Model to use (default: meta-llama/Meta-Llama-3.1-405B)
--train_samples   Limit training examples (for testing)
--test_samples    Limit test examples (for testing)
--max_icm_iters   Number of ICM iterations (default: 500)
--k               In-context examples (default: 6)
--seed            Random seed (default: 42)
```

## Expected Results

On full dataset (256 train, 100 test):
- Random Baseline: ~58%
- Zero-shot Baseline: ~60%
- ICM: ~85-90%
- Golden (upper bound): 100%

## Files

- `run_icm.py` - Main script
- `icm_algorithm.py` - ICM Algorithm 1 implementation
- `critique.md` - Paper critique (Part 2)
- `data/` - TruthfulQA train/test data

## Notes

- Uses BASE model (not instruct) for unbiased probabilities
- No fine-tuning (prompt-based only)
- Each run is saved separately (no overwrites)
